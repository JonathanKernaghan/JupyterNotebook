{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BasicFNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOcS4RRZK6VcOjGfc3xx8gI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonathanKernaghan/JupyterNotebook/blob/main/BasicFNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6bYZhDHkqAt"
      },
      "source": [
        "**Feedforward Artificial Neural Network (Basic)**\n",
        "There are 7 tasks to create a basic feedforward neural net and they are as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQoE8KQClVmS"
      },
      "source": [
        "1 ) Import Libraries & Set Device to GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_NeBXK2c2Uo"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnPld3N5ldLh"
      },
      "source": [
        "2) Define Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHfY19x5jt0n"
      },
      "source": [
        "batch_size = 100\n",
        "input_size = 784\n",
        "hidden_size = 100\n",
        "num_classes = 10\n",
        "num_epochs = 2\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0xxlN-9nBjd"
      },
      "source": [
        "3) Define datasets & loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Ump5Ulm8GZ"
      },
      "source": [
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV3wb4g9mEr4"
      },
      "source": [
        "4) Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-2eCyumk35W"
      },
      "source": [
        "model = nn.Sequential(\n",
        "        nn.Linear(input_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, num_classes)\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5EdVU5almnJ"
      },
      "source": [
        "5) Define Loss Function and Optimiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnKL89Zek_Uw"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4XneK6SlvDj"
      },
      "source": [
        "6) Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPMAQ3EkDjg"
      },
      "source": [
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DPC7cZul2K-"
      },
      "source": [
        "7) Testing Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0t0zde_ke_k"
      },
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        n_samples += labels.shape[0]\n",
        "        n_correct += (predictions == labels).sum().item()\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'accuracy = {acc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10s_HZ4En8Py"
      },
      "source": [
        "***The basic moving parts of a simple Feedforward Neural Network:***\n",
        "\n",
        "- The **architecture** of the model\n",
        "  - Number of hidden layers i.e. the depth\n",
        "  - Number of neurons in hidden layers i.e. the width\n",
        "  - Activation function used for neurons i.e. Sigmoid, ReLU, PReLU\n",
        "- **Loss function** used i.e. MSE, Cross Entropy Loss\n",
        "- **Optimization algorithm** used i.e. SGD, SGD w/ Momentum\n",
        "- **Learning rate** i.e. 0.001\n",
        "- **Epochs** i.e. runs through all samples\n",
        "- **Batch size**\n",
        "\n",
        "The above Neural Network is by no means optimal, and doesn't score well on MNIST. By adjusting the above apparatus, the accuracy will increase vastly. There are other techniques too, such as data augmentation - to jitter the digits to different angles and positions - and create more samples for the training set.\n",
        "\n",
        "Good post here explaining the additonal techniques and special features:\n",
        "\n",
        "[How to score 97%, 98%, 99%, and 100% in MNIST](https://www.kaggle.com/c/digit-recognizer/discussion/61480)\n",
        "\n",
        "I will create an optimised feedforward model in another notebook - to see how much accuracy I can gain using this non-convolutional linear architecture."
      ]
    }
  ]
}